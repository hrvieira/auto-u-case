# Classificador de E-mails: Produtivo vs. Improdutivo (AutoU Case)


[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/release/python-3100/)
[![Framework](https://img.shields.io/badge/Framework-FastAPI-green)](https://fastapi.tiangolo.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![CI/CD](https://img.shields.io/badge/CI/CD-GitHub_Actions-lightgrey)](.github/workflows/ci.yml)

Uma aplica√ß√£o web completa que classifica e-mails como **Produtivos** ou **Improdutivos**, sugerindo respostas autom√°ticas. Este projeto foi desenhado como uma solu√ß√£o robusta, escal√°vel e pronta para produ√ß√£o para o **Case Pr√°tico da AutoU**.

## Sum√°rio

- [‚ú® Principais Funcionalidades](#-principais-funcionalidades)
- [üèõÔ∏è Arquitetura da Solu√ß√£o](#arquitetura-da-solu√ß√£o)
- [üöÄ Come√ßando (Ambiente Local)](#-come√ßando-ambiente-local)
- [üîß Configura√ß√£o](#-configura√ß√£o)
- [üß† Treinando o Modelo de Machine Learning](#-treinando-o-modelo-de-machine-learning)
- [üéÆ API e Exemplos de Uso](#-api-e-exemplos-de-uso)
- [üß™ Testes Automatizados](#-testes-automatizados)
- [‚òÅÔ∏è Deploy e Escalabilidade](#deploy-e-escalabilidade)
- [üí° Poss√≠veis Melhorias](#-poss√≠veis-melhorias)
- [üìú Licen√ßa](#-licen√ßa)

---

## ‚ú® Principais Funcionalidades

* **Processamento Flex√≠vel:** Aceita e-mails via upload de arquivos (`.txt`, `.pdf`) ou texto simples.
* **Classifica√ß√£o H√≠brida:** Utiliza um modelo local (Scikit-learn) para classifica√ß√£o r√°pida e de baixo custo, com a op√ß√£o de fallback para um modelo avan√ßado via API da OpenAI para maior precis√£o.
* **Gera√ß√£o de Respostas:** Sugere respostas autom√°ticas com base em templates ou, opcionalmente, geradas pela OpenAI.
* **API Robusta:** Backend constru√≠do com FastAPI, garantindo alta performance, documenta√ß√£o autom√°tica (Swagger UI) e valida√ß√£o de dados.
* **Frontend Simples:** Interface em HTML puro e JavaScript para intera√ß√£o direta com a API, facilitando testes e demonstra√ß√µes.

---

## üèõÔ∏è Arquitetura da Solu√ß√£o

A solu√ß√£o segue uma arquitetura de microsservi√ßo desacoplada, com um backend servindo uma API RESTful e um frontend agn√≥stico que a consome.

```mermaid
graph TD
    subgraph "Cliente"
        A[Browser - index.html]
    end

    subgraph "Backend (FastAPI)"
        B(API Endpoint: /api/process)
        C{L√≥gica de Decis√£o}
        D[Modelo Local<br>(Scikit-learn)]
        E[API Externa<br>(OpenAI)]
    end

    A -- "Requisi√ß√£o HTTP (Upload ou Texto)" --> B
    B -- "Payload" --> C
    C -- "Se OPENAI_API_KEY existe" --> E
    C -- "Sen√£o" --> D
    D -- "Classifica√ß√£o/Resposta" --> B
    E -- "Classifica√ß√£o/Resposta" --> B
    B -- "Resposta JSON" --> A

    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#bbf,stroke:#333,stroke-width:2px
```

* **Frontend (`frontend/`):** Uma p√°gina est√°tica (`index.html`) com um formul√°rio que envia os dados para a API via `fetch`. Simples, leve e pode ser facilmente substitu√≠do por um framework moderno (React, Vue, etc.).
* **Backend (`backend/`):**
    * `main.py`: Ponto de entrada da aplica√ß√£o FastAPI. Define o endpoint `/api/process`.
    * `utils_text.py`: M√≥dulo de pr√©-processamento de texto (limpeza, tokeniza√ß√£o, lematiza√ß√£o) usando NLTK.
    * `ai_integration.py`: L√≥gica para interagir tanto com o modelo local (`.joblib`) quanto com a API da OpenAI.
    * `train_classifier.py`: Script para treinar o modelo de classifica√ß√£o (TF-IDF + Logistic Regression) a partir do dataset `data/examples.csv`.
* **Modelo de ML (`backend/models/`):** O `pipeline.joblib` cont√©m o pipeline treinado do Scikit-learn, pronto para ser carregado em produ√ß√£o sem necessidade de retreino.

---

## üöÄ Come√ßando (Ambiente Local)

Siga os passos abaixo para configurar e executar o projeto em sua m√°quina.

### Pr√©-requisitos

* Python 3.9+ (recomendado 3.10)
* `pip` e `venv` (geralmente inclusos no Python)
* Git
* (Opcional) Conta na OpenAI para obter uma `OPENAI_API_KEY`.

### Passos de Instala√ß√£o

1.  **Clone o Reposit√≥rio:**
    ```bash
    git clone https://github.com/<seu-usuario>/auto-u-case.git
    cd auto-u-case
    ```

2.  **Crie e Ative um Ambiente Virtual:**
    *√â uma boa pr√°tica isolar as depend√™ncias do projeto.*
    ```bash
    # Linux / macOS
    python3 -m venv venv
    source venv/bin/activate

    # Windows (PowerShell)
    python -m venv venv
    .\venv\Scripts\Activate
    ```

3.  **Instale as Depend√™ncias:**
    ```bash
    pip install -r backend/requirements.txt
    ```

4.  **Baixe os Recursos do NLTK:**
    *Necess√°rio para o pr√©-processamento de texto.*
    ```bash
    python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords'); nltk.download('wordnet')"
    ```

5.  **Configure as Vari√°veis de Ambiente:**
    *Copie o arquivo de exemplo e adicione suas chaves, se aplic√°vel.*
    ```bash
    cp .env.example .env
    ```
    Edite o arquivo `.env` para incluir sua `OPENAI_API_KEY` (opcional).

6.  **(Opcional) Treine o Modelo Local:**
    *Se voc√™ modificou `data/examples.csv` ou quer gerar o modelo pela primeira vez.*
    ```bash
    python backend/train_classifier.py
    ```
    Isso criar√° o arquivo `backend/models/pipeline.joblib`.

7.  **Execute o Servidor de Desenvolvimento:**
    ```bash
    uvicorn backend.main:app --reload --port 8000
    ```
    *A flag `--reload` reinicia o servidor automaticamente ap√≥s altera√ß√µes no c√≥digo, ideal para desenvolvimento.*

    Acesse a aplica√ß√£o em `http://localhost:8000/` ou a documenta√ß√£o interativa da API em `http://localhost:8000/docs`.

---

## üîß Configura√ß√£o

A configura√ß√£o da aplica√ß√£o √© feita atrav√©s de vari√°veis de ambiente, seguindo os princ√≠pios do [12-Factor App](https://12factor.net/config).

* **`.env.example`**: Um template que deve ser versionado no Git. **Nunca** adicione segredos a este arquivo.
    ```env
    # Modelo para configura√ß√£o. Copie para .env e preencha.
    OPENAI_API_KEY=sua_chave_aqui_se_for_usar
    OPENAI_MODEL=gpt-3.5-turbo
    MODEL_PATH=./backend/models/pipeline.joblib
    ```
* **`.env`**: Arquivo local para desenvolvimento. **Nunca** versione este arquivo no Git. Ele est√° inclu√≠do no `.gitignore` por padr√£o.
* **`.gitignore`**: Configurado para ignorar arquivos de ambiente, modelos treinados, caches do Python e outras pastas que n√£o devem ser versionadas.

---

## üß† Treinando o Modelo de Machine Learning

O script `backend/train_classifier.py` automatiza o processo de treinamento:

1.  Carrega os dados de `data/examples.csv`.
2.  Aplica o pr√©-processamento de texto definido em `utils_text.py`.
3.  Cria um pipeline do Scikit-learn com `TfidfVectorizer` e `LogisticRegression`.
4.  Treina o modelo e exibe um relat√≥rio de classifica√ß√£o (precis√£o, recall, F1-score).
5.  Salva o pipeline treinado em `backend/models/pipeline.joblib` usando `joblib` para otimiza√ß√£o.

Para treinar, basta executar:
```bash
python backend/train_classifier.py
```

---

## üéÆ API e Exemplos de Uso

### Endpoint: `POST /api/process`

Processa um e-mail para classifica√ß√£o e sugest√£o de resposta.

* **Content-Type:** `multipart/form-data`
* **Par√¢metros:**
    * `text` (string, opcional): O corpo do e-mail em texto puro.
    * `file` (file, opcional): Um arquivo `.txt` ou `.pdf` contendo o e-mail.
    * *Pelo menos um dos dois deve ser fornecido.*
* **Resposta de Sucesso (200 OK):**
    ```json
    {
      "category": "Produtivo",
      "confidence": 0.92,
      "suggested_response": "Entendido. Seu endere√ßo de cobran√ßa para o pedido 1234 foi atualizado com sucesso.",
      "preprocessed_excerpt": "ol√° preciso atualizar endere√ßo cobran√ßa pedido 1234"
    }
    ```

### Exemplos com `curl`

* **Enviando texto:**
    ```bash
    curl -X POST "http://localhost:8000/api/process" \
         -F "text=Ol√°, gostaria de confirmar o status de entrega do meu pedido #5678."
    ```
* **Enviando um arquivo:**
    ```bash
    curl -X POST "http://localhost:8000/api/process" \
         -F "file=@/path/to/your/email.txt"
    ```

### Exemplo com JavaScript `fetch` (para Frontend)

```javascript
const formData = new FormData();
formData.append('text', 'Preciso da segunda via do meu boleto, por favor.');

fetch('http://localhost:8000/api/process', {
  method: 'POST',
  body: formData
})
.then(response => response.json())
.then(data => console.log(data))
.catch(error => console.error('Error:', error));
```

---

## üß™ Testes Automatizados

O projeto utiliza `pytest` para testes unit√°rios e de integra√ß√£o.

* `tests/test_utils.py`: Testa as fun√ß√µes de pr√©-processamento de texto de forma isolada (testes unit√°rios).
* `tests/test_api.py`: Testa o endpoint `/api/process`, simulando requisi√ß√µes e validando as respostas (testes de integra√ß√£o).

Para executar todos os testes:
```bash
pytest -v
```

---

## ‚òÅÔ∏è Deploy e Escalabilidade

Esta aplica√ß√£o foi projetada para ser facilmente "containerizada" e implantada em qualquer provedor de nuvem moderno (AWS, GCP, Azure, Render, Heroku).

### 1. Containeriza√ß√£o com Docker

Um `Dockerfile` √© fornecido para criar uma imagem leve e otimizada da aplica√ß√£o.

<details>
<summary>üìÑ Exemplo de Dockerfile</summary>

```dockerfile
# Stage 1: Build com depend√™ncias completas
FROM python:3.10-slim as builder
WORKDIR /app
# Instala depend√™ncias de build (se necess√°rio) e do projeto
RUN pip install --upgrade pip
COPY backend/requirements.txt .
RUN pip wheel --no-cache-dir --wheel-dir /app/wheels -r requirements.txt

# Stage 2: Imagem final, mais leve
FROM python:3.10-slim
WORKDIR /app
# Copia as depend√™ncias pr√©-compiladas e o c√≥digo da aplica√ß√£o
COPY --from=builder /app/wheels /wheels
COPY backend/ .
COPY data/ ./data
COPY .env.example .
RUN pip install --no-cache /wheels/*
# Exp√µe a porta e define o comando de execu√ß√£o para produ√ß√£o
EXPOSE 8000
CMD ["gunicorn", "-w", "4", "-k", "uvicorn.workers.UvicornWorker", "backend.main:app", "-b", "0.0.0.0:8000"]
```
</details>

**Comandos Docker:**
```bash
# Construir a imagem
docker build -t email-classifier .
# Rodar o container
docker run -p 8000:8000 -v $(pwd)/.env:/app/.env email-classifier
```
*Servidor de Produ√ß√£o: Note que o Dockerfile utiliza `gunicorn` como servidor WSGI, que √© mais robusto para produ√ß√£o do que o servidor de desenvolvimento do `uvicorn`.*

### 2. Integra√ß√£o Cont√≠nua e Deploy Cont√≠nuo (CI/CD)

Um workflow de GitHub Actions (`.github/workflows/ci.yml`) pode ser configurado para automatizar testes e builds a cada push.

<details>
<summary>üìÑ Exemplo de Workflow de CI (GitHub Actions)</summary>

```yaml
name: CI Pipeline
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    - name: Set up Python 3.10
      uses: actions/setup-python@v3
      with:
        python-version: "3.10"
    - name: Install dependencies
      run: |
        pip install -r backend/requirements.txt
        python -m nltk.downloader punkt stopwords wordnet
    - name: Run tests
      run: pytest
```
</details>

### 3. Deploy em Plataformas (Ex: Render)

Plataformas como a Render simplificam o deploy. Voc√™ pode conectar seu reposit√≥rio Git e configurar o servi√ßo usando um arquivo `render.yaml`.

<details>
<summary>üìÑ Exemplo de render.yaml</summary>

```yaml
services:
  - type: web
    name: email-classifier
    env: python
    plan: free # ou o plano desejado
    buildCommand: |
      pip install -r backend/requirements.txt
      python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords'); nltk.download('wordnet')"
      # Se o modelo n√£o estiver versionado, treine-o aqui:
      # python backend/train_classifier.py
    startCommand: "gunicorn -w 4 -k uvicorn.workers.UvicornWorker backend.main:app -b 0.0.0.0:10000"
    envVars:
      - key: OPENAI_API_KEY
        fromSecret: true # Configure o segredo no dashboard da Render
```
</details>

---

## üí° Poss√≠veis Melhorias

Esta base s√≥lida pode ser estendida de v√°rias maneiras:

* **Frontend Avan√ßado:** Substituir o `index.html` por uma aplica√ß√£o em React, Vue ou Svelte para uma experi√™ncia de usu√°rio mais rica.
* **Modelos de ML mais sofisticados:** Experimentar modelos baseados em Transformers (ex: BERT, DistilBERT) para classifica√ß√£o de texto, usando bibliotecas como Hugging Face.
* **Cache de Respostas:** Implementar um cache com Redis para armazenar resultados de requisi√ß√µes id√™nticas, reduzindo a lat√™ncia e o custo com APIs externas.
* **Monitoramento e Logging:** Integrar ferramentas como Prometheus e Grafana para monitorar a sa√∫de da aplica√ß√£o e ELK/Loki para logging centralizado.
* **Banco de Dados:** Adicionar um banco de dados (ex: PostgreSQL) para salvar o hist√≥rico de e-mails processados e as classifica√ß√µes, permitindo auditoria e retreino cont√≠nuo do modelo.

---

## üìú Licen√ßa

Este projeto est√° licenciado sob a Licen√ßa MIT. Veja o arquivo `LICENSE` para mais detalhes.
